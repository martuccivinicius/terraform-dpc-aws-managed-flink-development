{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "import os\n",
    "import json\n",
    "from pyflink.table.expressions import *\n",
    "from pyflink.table.window import *\n",
    "\n",
    "# Line only for Local Development\n",
    "os.environ[\"IS_LOCAL\"] = \"True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Flink locally...\n",
      "file:////Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/lib/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/pomtest/target/aws-kinesis-analytics-python-apps-1.jar\n"
     ]
    }
   ],
   "source": [
    "# 1. Creates a Table Environment\n",
    "env_settings = EnvironmentSettings.in_streaming_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "statement_set = table_env.create_statement_set()\n",
    "\n",
    "APPLICATION_PROPERTIES_FILE_PATH = \"/etc/flink/application_properties.json\"  # on Kinesis Data Analytics\n",
    "\n",
    "is_local = (\n",
    "    True if os.environ.get(\"IS_LOCAL\") else False\n",
    ")  # set this env var in your local environment\n",
    "\n",
    "if is_local:\n",
    "    # only for local, overwrite variable to properties and pass in your jars delimited by a semicolon (;)\n",
    "    print(\"Running Flink locally...\")\n",
    "    APPLICATION_PROPERTIES_FILE_PATH = \"application_properties.json\"  # local\n",
    "\n",
    "    CURRENT_DIR = os.path.abspath(os.getcwd())\n",
    "    table_env.get_config().set(\n",
    "        \"pipeline.jars\",\n",
    "        f\"file:///{CURRENT_DIR}/lib/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/pomtest/target/aws-kinesis-analytics-python-apps-1.jar\")\n",
    "    print(f\"file:///{CURRENT_DIR}/lib/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/pomtest/target/flink-sql-connector-kinesis-1.15.4.jar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_application_properties():\n",
    "    if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH):\n",
    "        with open(APPLICATION_PROPERTIES_FILE_PATH, \"r\") as file:\n",
    "            contents = file.read()\n",
    "            properties = json.loads(contents)\n",
    "            return properties\n",
    "    else:\n",
    "        print('A file at \"{}\" was not found'.format(APPLICATION_PROPERTIES_FILE_PATH))\n",
    "\n",
    "\n",
    "def property_map(props, property_group_id):\n",
    "    for prop in props:\n",
    "        if prop[\"PropertyGroupId\"] == property_group_id:\n",
    "            return prop[\"PropertyMap\"]\n",
    "\n",
    "def create_source_table(table_name, stream_name, region, stream_initpos):\n",
    "    return \"\"\" CREATE TABLE IF NOT EXISTS {0} (\n",
    "                ticker VARCHAR(6),\n",
    "                price DOUBLE,\n",
    "                event_time TIMESTAMP(3),\n",
    "                WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n",
    "\n",
    "              )\n",
    "              PARTITIONED BY (ticker)\n",
    "              WITH (\n",
    "                'connector' = 'kinesis',\n",
    "                'stream' = '{1}',\n",
    "                'aws.region' = '{2}',\n",
    "                'scan.stream.initpos' = '{3}',\n",
    "                'format' = 'json',\n",
    "                'json.timestamp-format.standard' = 'ISO-8601'\n",
    "              ) \"\"\".format(\n",
    "        table_name, stream_name, region, stream_initpos\n",
    "    )\n",
    "\n",
    "\n",
    "def create_print_table(table_name, stream_name, region, stream_initpos):\n",
    "    return \"\"\" CREATE TABLE IF NOT EXISTS {0} (\n",
    "                ticker VARCHAR(6),\n",
    "                price DOUBLE,\n",
    "                event_time TIMESTAMP(3),\n",
    "                WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n",
    "\n",
    "              )\n",
    "              WITH (\n",
    "                'connector' = 'print'\n",
    "              ) \"\"\".format(\n",
    "        table_name, stream_name, region, stream_initpos\n",
    "    )\n",
    "\n",
    "def create_output_table(table_name, stream_name, region):\n",
    "    return \"\"\" CREATE TABLE IF NOT EXISTS {0} (\n",
    "                ticker VARCHAR(6),\n",
    "                start_timestamp TIMESTAMP(3),\n",
    "                end_timestamp TIMESTAMP(3),\n",
    "                rowtime_timestamp TIMESTAMP(3),\n",
    "                average_price DOUBLE\n",
    "\n",
    "              )\n",
    "              PARTITIONED BY (ticker)\n",
    "              WITH (\n",
    "                'connector' = 'kinesis',\n",
    "                'stream' = '{1}',\n",
    "                'aws.region' = '{2}',\n",
    "                'sink.partitioner-field-delimiter' = ';',\n",
    "                'sink.batch.max-size' = '100',\n",
    "                'format' = 'json',\n",
    "                'json.timestamp-format.standard' = 'ISO-8601'\n",
    "              ) \"\"\".format(\n",
    "        table_name, stream_name, region\n",
    "    )\n",
    "\n",
    "def create_fake_source_table(table_name):\n",
    "    return \"\"\" CREATE TABLE IF NOT EXISTS {0} (\n",
    "                ticker VARCHAR(6),\n",
    "                price DOUBLE,\n",
    "                event_time TIMESTAMP(3),\n",
    "                WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n",
    "\n",
    "              )\n",
    "              PARTITIONED BY (ticker)\n",
    "              WITH (\n",
    "                'connector' = 'datagen',\n",
    "                'number-of-rows' = '10'\n",
    "              ) \"\"\".format(table_name)\n",
    "\n",
    "def create_sink_table_s3(table_name, bucket_name):\n",
    "    return \"\"\" CREATE TABLE {0} (\n",
    "                ticker VARCHAR(6),\n",
    "                price DOUBLE,\n",
    "                event_time TIMESTAMP(3)\n",
    "              )\n",
    "              PARTITIONED BY (ticker)\n",
    "              WITH (\n",
    "                  'connector'='filesystem',\n",
    "                  'path'='s3a://{1}/',\n",
    "                  'format'='json',\n",
    "                  'sink.partition-commit.policy.kind'='success-file',\n",
    "                  'sink.partition-commit.delay' = '1 min'\n",
    "              ) \"\"\".format(table_name, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating source table input_table from source stream martucci-kinesis-stream-firehose in region us-east-1 using LATEST\n",
      " Creating print table output_table from source stream martucci-kinesis-stream-firehose in region us-east-1.\n",
      " Creating destination table output_table to destination stream martucci-kinesis-destination in region us-east-1.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8.executeSql.\n: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.input_table'.\n\nTable options are:\n\n'aws.region'='us-east-1'\n'connector'='kinesis'\n'format'='json'\n'json.timestamp-format.standard'='ISO-8601'\n'scan.stream.initpos'='LATEST'\n'stream'='martucci-kinesis-stream-firehose'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:219)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:244)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3997)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2867)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2427)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2341)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2286)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:723)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:709)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3843)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:617)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:229)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:205)\n\tat org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:69)\n\tat org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)\n\tat org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:73)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:272)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNodeOrFail(SqlNodeToOperationConversion.java:390)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertSqlInsert(SqlNodeToOperationConversion.java:745)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:353)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:262)\n\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='kinesis'\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:798)\n\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:215)\n\t... 33 more\nCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kinesis' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n\nAvailable factory identifiers are:\n\nblackhole\ndatagen\nfilesystem\nprint\npython-input-format\n\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:608)\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:794)\n\t... 35 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m table_env\u001b[39m.\u001b[39mexecute_sql(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     create_output_table(output_table_name,output_stream, output_region)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# 5. Insert from Source to Destination/Print\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m table_result \u001b[39m=\u001b[39m table_env\u001b[39m.\u001b[39;49mexecute_sql(\u001b[39m\"\u001b[39;49m\u001b[39mINSERT INTO \u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m SELECT * FROM \u001b[39;49m\u001b[39m{1}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                                          \u001b[39m.\u001b[39;49mformat(output_table_name, input_table_name))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viniciusdeoliveiramartucci/Documents/GitHub/martucci-glue-streaming/auxiliar-scripts/apache-flink/tutorial-aws-flink/kinesis-stream-to-stream/kinesis-stream-stream.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m table_result\u001b[39m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyflink/table/table_environment.py:837\u001b[0m, in \u001b[0;36mTableEnvironment.execute_sql\u001b[0;34m(self, stmt)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[39mExecute the given single statement, and return the execution result.\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[39m.. versionadded:: 1.11.0\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_execute()\n\u001b[0;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m TableResult(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_j_tenv\u001b[39m.\u001b[39;49mexecuteSql(stmt))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    147\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mpyflink\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjava_gateway\u001b[39;00m \u001b[39mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8.executeSql.\n: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.input_table'.\n\nTable options are:\n\n'aws.region'='us-east-1'\n'connector'='kinesis'\n'format'='json'\n'json.timestamp-format.standard'='ISO-8601'\n'scan.stream.initpos'='LATEST'\n'stream'='martucci-kinesis-stream-firehose'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:219)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:244)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3997)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2867)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2427)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2341)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2286)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:723)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:709)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3843)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:617)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:229)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:205)\n\tat org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:69)\n\tat org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)\n\tat org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:73)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:272)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNodeOrFail(SqlNodeToOperationConversion.java:390)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertSqlInsert(SqlNodeToOperationConversion.java:745)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:353)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:262)\n\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='kinesis'\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:798)\n\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:215)\n\t... 33 more\nCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kinesis' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n\nAvailable factory identifiers are:\n\nblackhole\ndatagen\nfilesystem\nprint\npython-input-format\n\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:608)\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:794)\n\t... 35 more\n"
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "# Application Property Keys\n",
    "input_property_group_key = \"consumer.config.0\"\n",
    "producer_property_group_key = \"producer.config.0\"\n",
    "\n",
    "input_stream_key = \"input.stream.name\"\n",
    "input_region_key = \"aws.region\"\n",
    "input_starting_position_key = \"flink.stream.initpos\"\n",
    "\n",
    "output_stream_key = \"output.stream.name\"\n",
    "output_region_key = \"aws.region\"\n",
    "\n",
    "# tables\n",
    "input_table_name = \"input_table\"\n",
    "output_table_name = \"output_table\"\n",
    "\n",
    "# get application properties\n",
    "props = get_application_properties()\n",
    "\n",
    "input_property_map = property_map(props, input_property_group_key)\n",
    "output_property_map = property_map(props, producer_property_group_key)\n",
    "\n",
    "input_stream = input_property_map[input_stream_key]\n",
    "input_region = input_property_map[input_region_key]\n",
    "stream_initpos = input_property_map[input_starting_position_key]\n",
    "\n",
    "output_stream = output_property_map[output_stream_key]\n",
    "output_region = output_property_map[output_region_key]\n",
    "\n",
    "# 0. Creates fake source table from datagen\n",
    "input_table_fake_name = \"fake_source_table\"\n",
    "table_env.execute_sql(\n",
    "    create_fake_source_table(input_table_fake_name)\n",
    ")\n",
    "\n",
    "# 2. Creates a source table from a Kinesis Data Stream\n",
    "print(f\" Creating source table {input_table_name} from source stream {input_stream} in region {input_region} using {stream_initpos}\")\n",
    "table_env.execute_sql(\n",
    "    create_source_table(input_table_name, input_stream, input_region, stream_initpos)\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Creates a print to check Data\n",
    "print_output_table = output_table_name + \"_print\"\n",
    "print(f\" Creating print table {output_table_name} from source stream {input_stream} in region {input_region}.\")\n",
    "table_env.execute_sql(\n",
    "    create_print_table(print_output_table, output_stream, output_region, stream_initpos)\n",
    ")\n",
    "\n",
    "# 4. Creates a sink table writing to a Kinesis Firehose Delivery Strem\n",
    "print(f\" Creating destination table {output_table_name} to destination stream {output_stream} in region {output_region}.\")\n",
    "table_env.execute_sql(\n",
    "    create_output_table(output_table_name,output_stream, output_region)\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Insert from Source to Destination/Print\n",
    "table_result = table_env.execute_sql(\"INSERT INTO {0} SELECT * FROM {1}\"\n",
    "                                         .format(output_table_name, input_table_name))\n",
    "\n",
    "table_result.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
